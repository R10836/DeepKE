model_args:
  model_name_or_path: 'models/llama2-13b-chat'
  model_name: 'llama'
  template: 'llama2'

data_args:
  train_file: 'data/train.json'
  valid_file: 'data/dev.json'

training_args:
  output_dir: 'lora/llama2-13b-chat-v1'
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  preprocessing_num_workers: 16
  num_train_epochs: 10
  learning_rate: 5e-5
  max_grad_norm: 0.5
  optim: 'adamw_torch'
  max_source_length: 400
  cutoff_len: 700
  max_target_length: 300
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  save_total_limit: 10
  bf16: true
  bits: 4
  deepspeed: None

finetuning_args:
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05

generating_args: {}
