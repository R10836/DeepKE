do_train: true
do_eval: true
overwrite_output_dir: true
model_name_or_path: 'models/llama2-13B-Chat'
stage: 'sft'
model_name: 'llama'
template: 'llama2'
train_file: 'data/train.json'
valid_file: 'data/dev.json'
output_dir: '${output_dir}'
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4
preprocessing_num_workers: 16
num_train_epochs: 10
learning_rate: 5e-5
max_grad_norm: 0.5
optim: 'adamw_torch'
max_source_length: 400
cutoff_len: 700
max_target_length: 300
evaluation_strategy: 'epoch'
save_strategy: 'epoch'
save_total_limit: 10
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
bf16: true
bits: 4