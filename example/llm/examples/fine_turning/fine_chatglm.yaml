output_dir: 'lora/chatglm3-6b-v1'
do_train: true
do_eval: true
overwrite_output_dir: true
stage: 'sft'
model_name_or_path: 'models/chatglm3-6b'
model_name: 'chatglm'
template: 'chatglm3'
train_file: 'data/NER/train.json'
valid_file: 'data/dev.json' # 补全的参数
val_set_size: 100
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4
preprocessing_num_workers: 16
num_train_epochs: 10
learning_rate: 5e-5
max_grad_norm: 0.5
optim: "adamw_torch"
max_source_length: 400
cutoff_len: 700
max_target_length: 300
evaluation_strategy: "epoch"
save_strategy: "epoch"
save_total_limit: 10
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
bf16: true
bits: 4
