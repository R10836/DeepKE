output_dir: 'lora/oneke-continue'
do_train: true
do_eval: true
overwrite_output_dir: true
stage: 'sft'
model_name_or_path: 'models/OneKE'
model_name: 'llama'
template: 'llama2_zh'
train_file: 'data/train.json'
valid_file: 'data/dev.json'
val_set_size: 100
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4
preprocessing_num_workers: 16
num_train_epochs: 10
learning_rate: 5e-5
max_grad_norm: 0.5
optim: "adamw_torch"
max_source_length: 400
cutoff_len: 700
max_target_length: 300
evaluation_strategy: "epoch"
save_strategy: "epoch"
save_total_limit: 10
lora_r: 64
lora_alpha: 64
lora_dropout: 0.05
bf16: true
bits: 4